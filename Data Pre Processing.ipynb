{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef745fa4",
   "metadata": {
    "id": "ef745fa4"
   },
   "source": [
    "<p>DATA PREPROCESSING WITH GOOGLE COLAB </p>\n",
    "<a href=\"https://colab.research.google.com/drive/15zB3v1PRyaLupEHQoKzSXA-b4Pvp0g4P?usp=sharing\">Colab Version</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea55540",
   "metadata": {
    "id": "0ea55540"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk bs4 wget elasticsearch symspellpy==6.7.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c9830c",
   "metadata": {
    "id": "39c9830c"
   },
   "outputs": [],
   "source": [
    "# dependency modules for NLP\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# dependency modules for elastic search & data preprocess\n",
    "from pandas.api.types import is_list_like,is_string_dtype\n",
    "from elasticsearch import Elasticsearch\n",
    "from urllib.request import urlopen\n",
    "from elasticsearch import helpers\n",
    "from functools import lru_cache\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import wget\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# dependency module for Spell Checker\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e2f301e",
   "metadata": {
    "id": "1e2f301e",
    "outputId": "b3dd3708-d69c-453a-90c0-b7f7a7ad11c7"
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4dd30bd",
   "metadata": {
    "id": "a4dd30bd"
   },
   "outputs": [],
   "source": [
    "class Data_Preprocessing:\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.all_text = '' \n",
    "        self.brands = []\n",
    "        self.main_cats = [] \n",
    "        self.my_symspell = SymSpell()\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tagger = PerceptronTagger() \n",
    "        self.p_stemmer = PorterStemmer()\n",
    "        self.counter = itertools.count(1)\n",
    "        self.stemmer = lru_cache(maxsize=50000)(self.p_stemmer.stem)\n",
    "        self.lemmatize = lru_cache(maxsize=50000)(self.wnl.lemmatize)\n",
    "        self.date_dict = {'january':'01','february':'02','march':'03','april':'04','may':'05','june':'06','july':'07','august':'08',\n",
    "                          'september':'09','october':'10','november':'11','december':'12'}\n",
    "    \n",
    "    # functions for text processing\n",
    "    def db_rm_punctuations(self,text):\n",
    "        temp = []\n",
    "        punctuations = re.compile(r'[!\"#%&\\'()*+,-./:;<=>?$@\\[\\\\\\]^_`{|}~]')\n",
    "        for words in text.split():\n",
    "            word = punctuations.sub(\" \",words)\n",
    "            temp.append(word)\n",
    "        return \" \".join(temp)\n",
    "    \n",
    "    def db_rm_stopwords(self,text):\n",
    "        temp = []\n",
    "        nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "        for word in text.split():\n",
    "            if word.lower() not in nltk_stop_words:  \n",
    "                temp.append(word)\n",
    "        return \" \".join(temp)\n",
    "\n",
    "    def db_rm_duplicates(self,text):\n",
    "        words = text.split()\n",
    "        return \" \".join(sorted(set(words), key=words.index))\n",
    "\n",
    "    def db_iso_nums(self,text):\n",
    "        pattern = re.compile(r'$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$') \n",
    "        new_word = pattern.sub(\"\",text)\n",
    "        if len(new_word) > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def db_get_wordnet_pos(self,tag):\n",
    "        if tag[0]=='J':\n",
    "            return wordnet.ADJ\n",
    "        elif  tag[0]=='V':\n",
    "            return wordnet.VERB\n",
    "        elif  tag[0]=='N':\n",
    "            return wordnet.NOUN\n",
    "        elif tag[0]=='R':\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    def db_lemmatization(self,text,lemmatizer=None,tagger=None):\n",
    "        if lemmatizer==None:\n",
    "            lemmatizer=self.lemmatize\n",
    "        if tagger==None:\n",
    "            tagger = self.tagger\n",
    "        temp = []\n",
    "        for word in text.split():\n",
    "            wn_pos = self.db_get_wordnet_pos(tagger.tag([word])[0][1])\n",
    "            if wn_pos != '':\n",
    "                temp.append(lemmatizer(word,pos=wn_pos))\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        return \" \".join(temp)\n",
    "\n",
    "    def db_stemming(self,text,stemmer=None):\n",
    "        if stemmer==None:\n",
    "            stemmer=self.stemmer\n",
    "        temp = []\n",
    "        for word in text.split():\n",
    "            temp.append(stemmer(word))\n",
    "        return \" \".join(temp)\n",
    "\n",
    "    def db_lower_case(self,text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def db_cvt2num(self,text):\n",
    "        text = text.replace('$','')\n",
    "        text = text.replace(',','')\n",
    "        if text.find('-'):\n",
    "            text = text.split('-')[0]\n",
    "        return float(text)\n",
    "\n",
    "    def db_rm_html_tags(self,text):\n",
    "        if type(text)==float:\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(text)\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def db_transdate(self,text):\n",
    "        try:\n",
    "            date=text.split()\n",
    "            if date!=[]:\n",
    "                date[1]=date[1].replace(',','')\n",
    "                if len(date[1])<2:\n",
    "                    date[1]='0'+date[1]\n",
    "                return date[2]+'-'+self.date_dict.get(date[0])+'-'+date[1]\n",
    "            else:\n",
    "                return '1994-07-05'\n",
    "        except:\n",
    "            return '1994-07-05'\n",
    "    \n",
    "    def db_getrank(self,text):\n",
    "        if text!='' and text!='[]':\n",
    "            rank = re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\",text.split('in')[0])\n",
    "            if not rank:\n",
    "              return 5000000\n",
    "            rank = ''.join(rank)\n",
    "            return int(rank)\n",
    "        else:\n",
    "            return 5000000\n",
    "\n",
    "    def db_dict2string(self,text):\n",
    "        return re.sub(' +',' ',str(text).strip('{}').replace('\\n','').replace('\\\\n',''))\n",
    "\n",
    "    # functions for creating custom dictionary & bigrams (spell checking & autocomplete)\n",
    "    def create_dict(self,seq,new=False):\n",
    "        if new==True:\n",
    "            self.my_symspell = SymSpell()\n",
    "        for word in seq:\n",
    "            self.my_symspell.create_dictionary_entry(word,1)\n",
    "        return True      \n",
    "    \n",
    "    def save_dict(self,filename=\"custom_dictionary.txt\"):\n",
    "        self.my_symspell.save_pickle(filename)\n",
    "        return True\n",
    "\n",
    "    def load_dict(self,filename=\"custom_dictionary.txt\"):\n",
    "        self.my_symspell.load_pickle(filename)\n",
    "        return True\n",
    "    \n",
    "    def save_text(self,filename=\"compile_text.txt\"):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.all_text, f)\n",
    "\n",
    "    def load_text(self,filename=\"compile_text.txt\"):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.all_text = pickle.load(f) \n",
    "\n",
    "    def compile_text(self,df,include_columns=['title']):\n",
    "        words = ''\n",
    "        for i in include_columns:\n",
    "            for j in df[i]:\n",
    "                words += j\n",
    "        self.all_text+=words\n",
    "    \n",
    "\n",
    "    def custom_dictionary(self,df,include_columns=['description','title','brand','main_cat']):\n",
    "        # extract vocabulary in dataset\n",
    "        custom_vocab = []\n",
    "        for i in include_columns:\n",
    "            for j in df[i]:\n",
    "                filt_words = [x for x in j.split() if self.db_iso_nums(x)]\n",
    "                custom_vocab.extend(filt_words)\n",
    "        unique_vocab = list(set(custom_vocab))\n",
    "        \n",
    "        # create dictionary\n",
    "        self.create_dict(seq=unique_vocab)\n",
    "\n",
    "    def build_ngrams(self,source=\"compile_text.txt\",dest=\"ngrams_freq.txt\",min_len=1,max_len=3):\n",
    "        with open(source, 'rb') as f:\n",
    "            words = pickle.load(f)\n",
    "\n",
    "        # tokenize words\n",
    "        tokens = nltk.word_tokenize(words)\n",
    "        # create ngrams\n",
    "        ngrams = nltk.everygrams(tokens,max_len=max_len,min_len=min_len)\n",
    "        # compute frequency distribution for all ngrams\n",
    "        fdist = nltk.FreqDist(ngrams)\n",
    "\n",
    "        # save ngrams frequency\n",
    "        with open(dest, 'wb') as f:\n",
    "            pickle.dump(fdist, f)\n",
    "        return True\n",
    "\n",
    "    ########## Text Preprocessing #############\n",
    "    def custom_text_process(self,df):\n",
    "        # keeping necessary attributes\n",
    "        df.drop_duplicates(subset=['asin'], keep='first',inplace=True)\n",
    "        df['price'].replace(\"\", np.nan, inplace=True)\n",
    "        df=df.loc[df['price'].str.find('$') == 0]\n",
    "        df=df[df['imageURLHighRes'].notna()]\n",
    "        df=df.loc[df['imageURLHighRes'].map(len) > 0]\n",
    "        df.drop(columns=['category', 'tech1','tech2','feature','fit','similar_item'],axis=1,errors='ignore',inplace=True)\n",
    "\n",
    "        # convert list columns to str\n",
    "        for i in df.columns:\n",
    "            if (is_list_like(df[i].iloc[0]) or i in ['description','also_buy','also_view','imageURL','imageURLHighRes']) and i != 'details':\n",
    "                df[i] = [' '.join(map(str, [\"\"])) if type(l)!=list else ' '.join(map(str, l)) for l in df[i]]\n",
    "        \n",
    "        # remove html tags\n",
    "        for i in ['description','title','main_cat','date','brand']:\n",
    "            df[i] = df[i].apply(self.db_rm_html_tags)\n",
    "\n",
    "        # text preprocessing\n",
    "        for i in df.columns:\n",
    "            if type(df[i].iloc[0])==str or i=='rank':\n",
    "                df[i] = df[i].apply(str)\n",
    "                if i not in ['imageURL','imageURLHighRes']:\n",
    "                    df[i] = df[i].apply(self.db_lower_case)\n",
    "            if i in ['description','title','brand']:\n",
    "                if i == 'description':\n",
    "                    df['orig_desc'] = df[i]\n",
    "                elif i == 'title':\n",
    "                    df['orig_title'] = df[i]\n",
    "                elif i == 'brand':\n",
    "                    df['orig_brand'] = df[i]\n",
    "                df[i] = df[i].apply(self.db_rm_punctuations)\n",
    "                df[i] = df[i].apply(self.db_rm_stopwords)\n",
    "                df[i] = df[i].apply(self.db_rm_duplicates)\n",
    "            if i == 'main_cat':\n",
    "                df[i] = df[i].apply(self.db_rm_punctuations)\n",
    "                df['orig_mcat'] = df[i]\n",
    "            if i == 'rank':\n",
    "                df[i] = df[i].apply(self.db_getrank)\n",
    "            if i == 'price':\n",
    "                df[i] = df[i].apply(self.db_cvt2num)\n",
    "            if i == 'details':\n",
    "                df[i] = df[i].apply(self.db_dict2string)\n",
    "            if i == 'date':\n",
    "                df[i] = df[i].apply(self.db_transdate)\n",
    "        \n",
    "        # create custom dictionary\n",
    "        self.custom_dictionary(df)\n",
    "        \n",
    "        # compile all text\n",
    "        self.compile_text(df)\n",
    "\n",
    "        # extract main categories\n",
    "        for i in df['main_cat'].unique():\n",
    "            for j in i.split():\n",
    "                if len(j)>1:\n",
    "                    self.main_cats.append(j)\n",
    "        self.main_cats = list(set(self.main_cats))\n",
    "        \n",
    "        # extract brands\n",
    "        for i in df['brand'].unique():\n",
    "            for j in i.split():\n",
    "                if len(j)>1:\n",
    "                    self.brands.append(j)\n",
    "        self.brands = list(set(self.brands))\n",
    "        \n",
    "        # additional text preprocessing\n",
    "        for i in ['description','title','brand','main_cat']:\n",
    "            df[i] = df[i].apply(self.db_stemming)\n",
    "            df[i] = df[i].apply(self.db_lemmatization)\n",
    "            df[i] = df[i].apply(self.db_rm_duplicates)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # indexing values to database\n",
    "    def df_to_db_eltk(self,df,es,idxname,chunk_size=250):\n",
    "        df_list = df.values.tolist()\n",
    "        docs =  [{\"_index\":idxname,\"_type\":\"product\",\"_id\":next(self.counter),\"_source\":{'desc':row[0],'title':row[1],\n",
    "                'also_buy':row[2],'brand':row[3],'rank':row[4],'rating':row[5],'also_view':row[6],'details':row[7],'main_cat':row[8],\n",
    "                'date':row[9],'price':row[10],'asin':row[11],'imageURL':row[12],'imageURLHighRes':row[13],\n",
    "                'orig_desc':row[14],'orig_title':row[15],'orig_brand':row[16],'orig_mcat':row[17]}\n",
    "            }for row in df_list]\n",
    "        helpers.bulk(client=es, actions=docs, chunk_size=chunk_size, request_timeout=600)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe288e",
   "metadata": {
    "id": "41fe288e"
   },
   "source": [
    "<h6>NOTE: DATA PREPROCESSING MAY TAKE SEVERAL HOURS, LEAVING ASIDE THE DOWNLOADING TIME OF THE DATASET</h6>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f25325a",
   "metadata": {
    "id": "4f25325a"
   },
   "outputs": [],
   "source": [
    "base_dir = \"C:/Users/user/Python Codes/Project Codes (THESIS)/Data Preprocess Small Complete\"\n",
    "meta_data_names = ['meta_Appliances.json.gz','meta_Cell_Phones_and_Accessories.json.gz','meta_Luxury_Beauty.json.gz','meta_Video_Games.json.gz']\n",
    "meta_data_names.extend(['meta_Clothing_Shoes_and_Jewelry.json.gz','meta_Home_and_Kitchen.json.gz','meta_Electronics.json.gz'])\n",
    "dictionary_dir = base_dir+\"resources/\"+\"custom_dictionary.txt\"\n",
    "main_cats_dir = base_dir+\"resources/\"+\"main_cats.txt\"\n",
    "ngrams_dir = base_dir+\"resources/\"+\"ngrams_freq.txt\"\n",
    "text_dir = base_dir+\"resources/\"+\"compile_text.txt\"\n",
    "brands_dir = base_dir+\"resources/\"+\"brands.txt\"\n",
    "my_db_class = None\n",
    "\n",
    "# for name in meta_data_names:\n",
    "#     # resources temp storage\n",
    "#     main_cats,brands = [],[]\n",
    "\n",
    "#     # create class instance (reset class data)\n",
    "#     my_db_class = Data_Preprocessing()\n",
    "    \n",
    "#     # load metadata\n",
    "#     data,lmt,prt,pure = [],0,1,name.split('.')[0]\n",
    "#     if os.path.exists(\"/content/{}\".format(name)) != True:\n",
    "#       wget.download(\"http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/{}\".format(name))\n",
    "    \n",
    "#     # load resources if exist (dictionary, main cats, brands)\n",
    "#     if os.path.isdir(base_dir+\"resources\") != True:\n",
    "#       os.mkdir(base_dir+\"resources\")\n",
    "#     # dictionary\n",
    "#     if os.path.exists(dictionary_dir):\n",
    "#       my_db_class.load_dict(filename=dictionary_dir)\n",
    "#     # main cats\n",
    "#     if os.path.exists(main_cats_dir):\n",
    "#       with open(main_cats_dir, 'rb') as f:\n",
    "#         main_cats = pickle.load(f)\n",
    "#     # brands\n",
    "#     if os.path.exists(brands_dir):\n",
    "#       with open(brands_dir, 'rb') as f:\n",
    "#         brands = pickle.load(f)\n",
    "        \n",
    "#     # Preprocessing\n",
    "#     with gzip.open(name) as f:\n",
    "#         if os.path.isdir(base_dir+pure) != True:\n",
    "#             os.mkdir(base_dir+pure)\n",
    "#         for l in f:\n",
    "#             if lmt==50000:\n",
    "#                 df = pd.DataFrame.from_dict(data)\n",
    "#                 st = time.time()\n",
    "#                 # load text\n",
    "#                 if os.path.exists(text_dir):\n",
    "#                     my_db_class.load_text(text_dir)\n",
    "#                 df = my_db_class.custom_text_process(df)\n",
    "#                 # save text\n",
    "#                 my_db_class.save_text(text_dir)\n",
    "#                 # save df\n",
    "#                 df.to_pickle(base_dir+pure+\"/\"+name+\"_\"+str(prt))\n",
    "#                 end = time.time()\n",
    "#                 print(\"Elapsed Time in Processing \"+name+\"_\"+str(prt)+\":\",end-st)\n",
    "#                 data,lmt = [],0\n",
    "#                 prt+=1\n",
    "#             data.append(json.loads(l.strip()))\n",
    "#             lmt+=1\n",
    "#         if lmt!=0:\n",
    "#             df = pd.DataFrame.from_dict(data)\n",
    "#             st = time.time()\n",
    "#             # load text\n",
    "#             if os.path.exists(text_dir):\n",
    "#                 my_db_class.load_text(text_dir)\n",
    "#             df = my_db_class.custom_text_process(df)\n",
    "#             # save text\n",
    "#             my_db_class.save_text(text_dir)\n",
    "#             # save df\n",
    "#             df.to_pickle(base_dir+pure+\"/\"+name+\"_\"+str(prt))\n",
    "#             end = time.time()\n",
    "#             print(\"Elapsed Time in Processing \"+name+\"_\"+str(prt)+\":\",end-st)\n",
    "            \n",
    "#         # save/update the dictionary\n",
    "#         my_db_class.save_dict(filename=dictionary_dir)\n",
    "\n",
    "#         # save/update main categories\n",
    "#         with open(main_cats_dir, 'wb') as f:\n",
    "#             main_cats.extend(my_db_class.main_cats)\n",
    "#             main_cats=list(set(main_cats))\n",
    "#             pickle.dump(main_cats, f)\n",
    "\n",
    "#         # save/update brands\n",
    "#         with open(brands_dir, 'wb') as f:\n",
    "#             brands.extend(my_db_class.brands)\n",
    "#             brands=list(set(brands))\n",
    "#             pickle.dump(brands, f)\n",
    "\n",
    "# print('Data Preprocessing Completed!')\n",
    "# print('Now building ngrams...')\n",
    "# my_db_class = Data_Preprocessing()\n",
    "# st = time.time()\n",
    "# my_db_class.build_ngrams(source=text_dir,dest=ngrams_dir,max_len=3)\n",
    "# end = time.time()\n",
    "# print(\"Elapsed Time in building ngrams:\",end-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e61444",
   "metadata": {
    "id": "f4e61444"
   },
   "source": [
    "Adding Amazon Mobile Phones Metadata from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75543889",
   "metadata": {
    "id": "75543889"
   },
   "outputs": [],
   "source": [
    "# # download dataset from https://www.kaggle.com/grikomsn/amazon-cell-phones-reviews\n",
    "# df = pd.read_csv('C:/Users/user/Python Codes/Project Codes (THESIS)/20191226-items.csv')\n",
    "# dictionary_dir = base_dir+\"resources/\"+\"custom_dictionary.txt\"\n",
    "\n",
    "# # load dictionary\n",
    "# my_db_class = Data_Preprocessing()\n",
    "# my_db_class.load_dict(filename=dictionary_dir)\n",
    "\n",
    "# df = pd.read_csv('20191226-items.csv')\n",
    "\n",
    "# if os.path.isdir(base_dir+\"meta_Mobile_Phones\") != True:\n",
    "#     os.mkdir(base_dir+\"meta_Mobile_Phones\")\n",
    "\n",
    "# nums = [i for i in range(1,len(df['title'])+1)]\n",
    "# random.shuffle(nums)\n",
    "\n",
    "# df.rename(columns={'image': 'imageURLHighRes'}, inplace=True)\n",
    "# df = df[['asin','brand','title','imageURLHighRes','price']]\n",
    "# avg = sum(df['price'])/len(df['price'])\n",
    "# df['main_cat'] = 'mobile phone smartphone'\n",
    "# df['date'] = '2020-01-01'\n",
    "# df['also_view'] = ''\n",
    "# df['also_buy'] = ''\n",
    "# df['imageURL'] = ''\n",
    "# df['details'] = ''\n",
    "# df['rank'] = nums\n",
    "\n",
    "# for i in df.columns:\n",
    "#     if i in ['asin','brand','title']:\n",
    "#         df[i] = df[i].apply(str)\n",
    "#         df[i] = df[i].apply(my_db_class.db_lower_case)\n",
    "#     if i in ['brand','title']:\n",
    "#         if i == 'title':\n",
    "#             df['orig_desc'] = df[i]\n",
    "#             df['orig_title'] = df[i]\n",
    "#         elif i == 'brand':\n",
    "#             df['orig_brand'] = df[i]\n",
    "#         df[i] = df[i].apply(my_db_class.db_rm_punctuations)\n",
    "#         df[i] = df[i].apply(my_db_class.db_rm_stopwords)\n",
    "#         df[i] = df[i].apply(my_db_class.db_rm_duplicates)\n",
    "#         if i == 'title':\n",
    "#             df['description'] = df[i]\n",
    "#     if i == 'price':\n",
    "#         df[i]=[x if x!=0.00 else round(avg,2) for x in df[i]]\n",
    "#     if i == 'main_cat':\n",
    "#         df['orig_mcat'] = df[i]\n",
    "\n",
    "# my_db_class.custom_dictionary(df)\n",
    "\n",
    "# for i in df.columns:\n",
    "#     if i in ['brand','title','main_cat']:\n",
    "#         df[i] = df[i].apply(my_db_class.db_check_pos_tag)\n",
    "#         df[i] = df[i].apply(my_db_class.db_stemming)\n",
    "#         df[i] = df[i].apply(my_db_class.db_lemmatization)\n",
    "#         df[i] = df[i].apply(my_db_class.db_rm_duplicates)\n",
    "        \n",
    "# df.to_pickle(base_dir+\"meta_Mobile_Phones/meta_Mobile_Phones.json.gz_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c6b00b",
   "metadata": {
    "id": "d3c6b00b"
   },
   "outputs": [],
   "source": [
    "# # add custom vocabs\n",
    "# my_db_class.create_dict(seq=['greater','higher','highest','lower','lowest','less', 'beyond', 'never', 'except', 'dollar', 'january', \n",
    "#                               'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'])\n",
    "# my_db_class.create_dict(seq=stopwords.words(\"english\"))\n",
    "# # save the updated dictionary\n",
    "# my_db_class.save_dict(filename=dictionary_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debc051",
   "metadata": {
    "id": "6debc051"
   },
   "source": [
    "<h6>RUN THIS CODE ON LOCAL MACHINE AFTER DOWNLOADING ELASTIC SEARCH (link available below)</h6>\n",
    "<a href=\"https://www.elastic.co/downloads/elasticsearch\">Elastic Search Link</a>\n",
    "\n",
    "<p>Step 1: download the elastic search window zip file</p>\n",
    "<p>Step 2: extract the zip file to your desired path e.g. \"C:\\Users\\your_user\\ElasticSearch\"</p>\n",
    "<p>Step 3: after extraction, navigate to the bin file path of elastic search folder  e.g. \"C:\\Users\\user\\ElasticSearch\\elasticsearch-7.17.0\\bin\"</p>\n",
    "<p>Step 4: add the path e.g. (\"C:\\Users\\user\\ElasticSearch\\elasticsearch-7.17.0\\bin\") to your system enviroment variables.\n",
    "to do that you can search and click 'edit system environment variables' on your computer\n",
    "then go to advance tab, click environment variables, in the System variables click 'path' and then click edit,\n",
    "click new then copy paste the path, click ok </p>\n",
    "<p>Step 5: run the elastic search server using cmd, open cmd and type 'elasticsearch' then enter </p>\n",
    "<p>Step 6: wait for the server to run, you can check if the server is up by opening a browser and navigate to <a href=http://localhost:9200>http://localhost:9200</a>\n",
    "the server is up when you see some information on the given url address</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21889b79",
   "metadata": {
    "id": "21889b79"
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(HOST=\"http://localhost\", PORT=9200)\n",
    "my_db_class = Data_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef43852",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete(index='amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb7f040b",
   "metadata": {
    "id": "cb7f040b",
    "outputId": "5ed94e30-0603-4efa-e488-5a3ae9befd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing: meta_Appliances.json.gz_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Appliances.json.gz_1: 33.59723997116089\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_1: 4.336954116821289\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_10: 17.57241129875183\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_11: 13.471562147140503\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_12: 70.69651460647583\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_2: 11.7816002368927\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_3: 9.186009168624878\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_4: 12.364008903503418\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_5: 9.738965272903442\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_6: 27.450361967086792\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_7: 19.834563970565796\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_8: 22.526806592941284\n",
      "Indexing: meta_Cell_Phones_and_Accessories.json.gz_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Cell_Phones_and_Accessories.json.gz_9: 25.642383098602295\n",
      "Indexing: meta_Luxury_Beauty.json.gz_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Luxury_Beauty.json.gz_1: 14.865371942520142\n",
      "Indexing: meta_Video_Games.json.gz_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Video_Games.json.gz_1: 6.214976072311401\n",
      "Indexing: meta_Video_Games.json.gz_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\NLP_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time in Indexing meta_Video_Games.json.gz_2: 6.206044435501099\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/user/Python Codes/Project Codes (THESIS)/Data Preprocess Small Complete/meta_Clothing_Shoes_and_Jewelry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7380/2584703897.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmeta_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmeta_data_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpure_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpure_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpure_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         df = df[['description','title','also_buy','brand','rank','rating','also_view','details','main_cat','date','price','asin',\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/user/Python Codes/Project Codes (THESIS)/Data Preprocess Small Complete/meta_Clothing_Shoes_and_Jewelry'"
     ]
    }
   ],
   "source": [
    "for meta_name in meta_data_names:\n",
    "    pure_name = meta_name.split(\".\")[0]\n",
    "    for name in os.listdir(base_dir+\"/\"+pure_name):\n",
    "        df = pd.read_pickle(base_dir+\"/\"+pure_name+\"/\"+name)\n",
    "        df = df[['description','title','also_buy','brand','rank','rating','also_view','details','main_cat','date','price','asin',\n",
    "                'imageURL','imageURLHighRes','orig_desc','orig_title','orig_brand','orig_mcat']]\n",
    "        print(\"Indexing:\",name)\n",
    "        st = time.time()\n",
    "        my_db_class.df_to_db_eltk(df=df,es=es,idxname='amazon',chunk_size=500)\n",
    "        end = time.time()\n",
    "        print(\"Elapsed Time in Indexing \"+name+\":\",end-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c9bbebd",
   "metadata": {
    "id": "9c9bbebd",
    "outputId": "7d68894a-e9e4-42fe-9efe-c52f040d599c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index            uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   amazon           GeOFGe11SQSpbvT2bmSvrw   1   1      74080            0    195.1mb        195.1mb\n",
      "green  open   .geoip_databases NXIVcwW9Q-6DPwcZMpBrJg   1   0         41            3     38.7mb         38.7mb\n",
      "yellow open   sample           lMw5E725RfCWkIaTpxYONg   1   1          3            0        5kb            5kb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:9200/_cat/indices?v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf472a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amazon': {'mappings': {'properties': {'also_buy': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'also_view': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'asin': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'brand': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'date': {'type': 'date'},\n",
       "    'desc': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'details': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'imageURL': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'imageURLHighRes': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'main_cat': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'orig_brand': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'orig_desc': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'orig_mcat': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'orig_title': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}},\n",
       "    'price': {'type': 'float'},\n",
       "    'rank': {'type': 'long'},\n",
       "    'rating': {'type': 'float'},\n",
       "    'title': {'type': 'text',\n",
       "     'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.get_mapping(index='amazon')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Thesis Data Pre Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
